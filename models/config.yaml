# ChatBI AI Model Configuration

# Model settings
model:
  type: "llama"  # Options: llama, gpt4all, openai
  name: "llama-2-7b-chat"
  path: "./models/llama/llama-2-7b-chat.gguf"
  context_length: 2048
  temperature: 0.7
  top_p: 0.9
  max_tokens: 512

# OpenAI settings (if using OpenAI)
openai:
  model: "gpt-3.5-turbo"
  api_key: "${OPENAI_API_KEY}"
  temperature: 0.7
  max_tokens: 512

# GPT4All settings (if using GPT4All)
gpt4all:
  model: "orca-mini-3b.gguf"
  path: "./models/gpt4all/orca-mini-3b.gguf"
  temperature: 0.7
  max_tokens: 512

# Query processing settings
query_processing:
  max_query_length: 500
  supported_languages: ["en", "fr"]
  preprocessing_enabled: true

# Response generation settings
response_generation:
  include_sql: true
  include_insights: true
  include_visualization_suggestions: true
  max_response_length: 1000

# Performance settings
performance:
  cache_enabled: true
  cache_ttl: 3600  # 1 hour
  max_concurrent_requests: 5

# Logging settings
logging:
  level: "INFO"
  log_queries: true
  log_responses: false  # Set to true for debugging